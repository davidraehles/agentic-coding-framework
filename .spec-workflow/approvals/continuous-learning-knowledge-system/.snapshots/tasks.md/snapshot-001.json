{
  "id": "snapshot_1760818128471_tkf1dwrg8",
  "approvalId": "approval_1760818128466_p759bmvtg",
  "approvalTitle": "Tasks Document - Implementation Plan for Continuous Learning Knowledge System",
  "version": 1,
  "timestamp": "2025-10-18T20:08:48.471Z",
  "trigger": "initial",
  "status": "pending",
  "content": "# Tasks Document: Continuous Learning Knowledge System\n\n**Spec**: `continuous-learning-knowledge-system`\n**Phase**: Implementation Planning\n**Created**: 2025-01-18\n**Status**: Ready for Implementation\n\n---\n\n## Implementation Phases\n\nThis document breaks down the design into atomic, implementable tasks organized by implementation phases. Each task includes:\n- **File**: Target file path\n- **Purpose**: What this task achieves\n- **Leverage**: Existing code/patterns to reuse\n- **Requirements**: Requirements document reference\n- **Prompt**: Detailed implementation guidance for AI coding assistant\n\n---\n\n## Phase 1: Foundation - Unified Inference Engine\n\n### [ ] 1.1 Create UnifiedInferenceEngine core class\n\n- **File**: `src/inference/UnifiedInferenceEngine.js`\n- **Purpose**: Create the central LLM inference engine shared by all components (trajectory, knowledge extraction, etc.)\n- **Leverage**: `integrations/mcp-constraint-monitor/src/engines/semantic-validator.js` (multi-provider pattern, circuit breaker, caching)\n- **Requirements**: FR-1.1, FR-1.2, FR-1.3\n- **Prompt**: Role: Senior Backend Developer specializing in LLM inference systems and multi-provider orchestration | Task: Create UnifiedInferenceEngine class that (1) supports local models via Ollama/vLLM and remote models via Groq/Anthropic/OpenAI, (2) implements circuit breaker pattern for provider failover, (3) integrates LRU caching with 40%+ hit rate, (4) routes sensitive content to local models automatically, (5) checks budget before remote inference, extending the multi-provider pattern from semantic-validator.js | Restrictions: Must not duplicate code from semantic-validator.js - extract and reuse common patterns; must support streaming responses; must handle provider-specific rate limits; must log all inference operations for cost tracking | Success: Engine successfully routes inference requests to appropriate providers, circuit breaker prevents cascading failures, caching reduces costs by 40%+, sensitive data never reaches remote APIs, budget limits are enforced\n\n### [ ] 1.2 Create BudgetTracker component\n\n- **File**: `src/inference/BudgetTracker.js`\n- **Purpose**: Track LLM costs in real-time, enforce configurable budget limits, provide cost analytics\n- **Leverage**: DuckDB for temporal cost tracking, existing cost estimation patterns\n- **Requirements**: FR-2.1, FR-2.2, FR-2.3\n- **Prompt**: Role: FinOps Engineer with expertise in cost tracking and resource management | Task: Implement BudgetTracker that (1) tracks all LLM API costs in DuckDB budget_events table, (2) enforces configurable monthly limits ($100/year default = $8.33/month), (3) estimates costs before API calls using token counting, (4) sends alerts at 80% threshold, (5) provides cost analytics by project/provider/operation, (6) automatically triggers fallback to local models when budget exceeded | Restrictions: Must persist all cost events to DuckDB for historical analysis; must support per-developer and per-project budget limits; cost estimation must be accurate within 10%; must not block development work when budget exceeded (graceful degradation to local models) | Success: All API costs tracked with <1% error rate, budget limits enforced automatically, developers receive timely alerts, cost analytics available for optimization decisions, system automatically uses local models when budget exceeded\n\n### [ ] 1.3 Create SensitivityClassifier component\n\n- **File**: `src/inference/SensitivityClassifier.js`\n- **Purpose**: Detect sensitive data using LSL 5-layer classification to route to local models\n- **Leverage**: `scripts/enhanced-transcript-monitor.js` (5-layer classification: SessionFilter, PathClassifier, KeywordClassifier, EmbeddingClassifier, SemanticAnalyzer)\n- **Requirements**: FR-3.1, FR-3.2, FR-3.3\n- **Prompt**: Role: Security Engineer with expertise in data classification and privacy protection | Task: Implement SensitivityClassifier that (1) reuses LSL 5-layer classification system for sensitivity detection, (2) supports configurable sensitivity topics in .specstory/config/sensitivity-topics.json, (3) classifies data as public/internal/confidential/secret, (4) integrates with UnifiedInferenceEngine to route sensitive data to local models, (5) provides confidence scores for classification, extending the layer architecture from enhanced-transcript-monitor.js | Restrictions: Must not send sensitive data to remote APIs even during classification; must complete classification in <100ms for real-time use; must support custom sensitivity rules per project; false negative rate must be <1% (better to over-classify as sensitive) | Success: Sensitive data correctly classified with >99% accuracy, classification completes in <100ms, zero sensitive data leaks to remote APIs, custom rules work correctly, integrates seamlessly with inference engine\n\n### [ ] 1.4 Create provider implementations\n\n- **File**: `src/inference/providers/GroqProvider.js`, `LocalProvider.js`, `AnthropicProvider.js`, `OpenAiProvider.js`\n- **Purpose**: Implement provider-specific adapters for LLM inference\n- **Leverage**: Existing API client patterns, provider SDK documentation\n- **Requirements**: FR-1.4, FR-1.5\n- **Prompt**: Role: Integration Engineer with expertise in API clients and SDK integration | Task: Create provider adapter classes that (1) implement consistent interface for all providers (infer, estimateCost, checkHealth), (2) handle provider-specific authentication and rate limits, (3) support streaming responses, (4) implement retry logic with exponential backoff, (5) provide accurate cost estimation per provider pricing, for Groq (llama-3.3-70b, qwen-2.5-32b), Anthropic (claude-3-haiku), OpenAI (gpt-4o-mini), and Local (Ollama/vLLM) | Restrictions: Must not hard-code API keys (use environment variables); must handle network failures gracefully; must log all API interactions for debugging; streaming must work for real-time trajectory updates | Success: All providers work correctly with consistent interface, streaming responses functional, cost estimation accurate within 5%, rate limits respected, authentication secure, retry logic prevents transient failures\n\n### [ ] 1.5 Create AgentAgnosticCache\n\n- **File**: `src/caching/AgentAgnosticCache.js`\n- **Purpose**: Replace MCP Memory with agent-agnostic caching solution supporting file, HTTP, and MCP backends\n- **Leverage**: Existing MCP Memory patterns, LRU cache implementation\n- **Requirements**: FR-4.1, FR-4.2\n- **Prompt**: Role: Cache Architect specializing in distributed caching and multi-backend systems | Task: Implement AgentAgnosticCache that (1) supports file-based caching for non-MCP agents, (2) supports HTTP API caching for remote agents, (3) supports MCP caching for Claude Code, (4) implements LRU eviction with configurable size limits, (5) provides cache statistics (hit rate, size, evictions), (6) works with any CLI-based coding agent (not just Claude), extending the caching patterns from MCP Memory but with multi-backend support | Restrictions: Must not depend on Claude-specific features; cache must be thread-safe for concurrent access; must handle cache corruption gracefully; cache keys must be deterministic and collision-free | Success: Cache works correctly with Claude, CoPilot, and other CLI agents; hit rate >40%; cache operations complete in <10ms; statistics accurate; no data corruption; seamless backend switching\n\n---\n\n## Phase 2: Database Infrastructure\n\n### [ ] 2.1 Create DatabaseManager for Qdrant + DuckDB\n\n- **File**: `src/databases/DatabaseManager.js`\n- **Purpose**: Unified interface for managing both Qdrant (vectors) and DuckDB (temporal/analytics)\n- **Leverage**: `integrations/mcp-constraint-monitor/src/databases/qdrant-client.js`\n- **Requirements**: FR-5.1, FR-5.2\n- **Prompt**: Role: Database Architect with expertise in vector databases and analytical databases | Task: Create DatabaseManager that (1) manages connections to both Qdrant and DuckDB, (2) provides unified transaction semantics across both databases, (3) handles connection pooling and health checks, (4) implements graceful degradation if databases unavailable, (5) provides query interfaces for both semantic search (Qdrant) and temporal queries (DuckDB), extending the Qdrant client patterns but adding DuckDB integration | Restrictions: Must handle database failures without blocking development; must support read replicas for scaling; must implement connection retry logic; must clean up connections on shutdown | Success: Both databases managed correctly with unified interface, transactions work across databases, health checks detect issues promptly, graceful degradation works, connection pooling improves performance\n\n### [ ] 2.2 Create Qdrant collections with dual vector sizes\n\n- **File**: `src/databases/QdrantCollectionManager.js`\n- **Purpose**: Create and manage Qdrant collections with 1024-dim for concepts, 384-dim for code patterns\n- **Leverage**: Existing Qdrant configuration (384-dim, HNSW, int8 quantization)\n- **Requirements**: FR-5.3, FR-5.4\n- **Prompt**: Role: Vector Database Engineer with expertise in Qdrant and embedding optimization | Task: Implement QdrantCollectionManager that (1) creates knowledge_concepts collection with 1024-dim vectors for accuracy, (2) creates code_patterns collection with 384-dim vectors for speed, (3) creates trajectory_intents collection with 384-dim vectors, (4) configures HNSW indexing (m=16, ef_construct=100), (5) enables int8 quantization for memory efficiency, (6) provides methods for upserting, searching, and managing collections, extending existing Qdrant configuration but with dual vector sizes | Restrictions: Must support incremental updates without full reindex; must handle vector dimension mismatches gracefully; must implement batch upsert for efficiency; HNSW parameters must balance accuracy and speed | Success: Collections created with correct configurations, search accuracy >95% for concepts, search speed <100ms for patterns, quantization reduces memory by 4x, batch operations efficient\n\n### [ ] 2.3 Create DuckDB schema and tables\n\n- **File**: `src/databases/DuckDBSchemaManager.js`\n- **Purpose**: Create and manage DuckDB tables for temporal queries (knowledge_events, trajectory_history, budget_events)\n- **Leverage**: DuckDB embedded database capabilities, existing schema patterns\n- **Requirements**: FR-5.5, FR-5.6\n- **Prompt**: Role: Data Engineer with expertise in DuckDB and time-series analytics | Task: Implement DuckDBSchemaManager that (1) creates knowledge_events table for tracking knowledge creation/modification/access, (2) creates trajectory_history table for session trajectory analysis, (3) creates budget_events table for cost tracking, (4) adds indexes on timestamp and project columns for query performance, (5) implements partitioning by month for historical data, (6) provides query methods for temporal analytics, using DuckDB's embedded analytical capabilities | Restrictions: Must support schema migrations for version upgrades; must handle concurrent writes safely; indexes must improve query performance by >10x; partitioning must support efficient historical queries | Success: All tables created with correct schema, indexes improve query performance >10x, concurrent writes work correctly, migrations tested, temporal queries execute in <500ms\n\n### [ ] 2.4 Create embedding generation service\n\n- **File**: `src/databases/EmbeddingGenerator.js`\n- **Purpose**: Generate embeddings using Transformers.js (384-dim for speed, 1024-dim for accuracy)\n- **Leverage**: Transformers.js library, LSL layer 3 embedding model (sentence-transformers/all-MiniLM-L6-v2)\n- **Requirements**: FR-5.7, FR-5.8\n- **Prompt**: Role: ML Engineer with expertise in transformer models and embedding generation | Task: Implement EmbeddingGenerator that (1) generates 384-dim embeddings using all-MiniLM-L6-v2 for speed-critical operations (code patterns, trajectories), (2) generates 1024-dim embeddings using larger model for accuracy-critical operations (concepts), (3) batches embedding generation for efficiency, (4) caches embeddings to avoid recomputation, (5) runs embedding generation in worker threads to avoid blocking, leveraging Transformers.js and LSL embedding patterns | Restrictions: Must not block main thread during generation; must handle out-of-memory errors gracefully; batch size must optimize throughput without exceeding memory; cache must use content hash for keys | Success: 384-dim embeddings generated in <50ms, 1024-dim in <200ms, batching improves throughput >5x, caching reduces redundant computation, worker threads prevent blocking\n\n---\n\n## Phase 3: Knowledge Extraction & Management\n\n### [ ] 3.1 Create StreamingKnowledgeExtractor agent\n\n- **File**: `integrations/mcp-server-semantic-analysis/src/agents/StreamingKnowledgeExtractor.ts`\n- **Purpose**: Extract knowledge from live session transcripts in real-time using streaming analysis\n- **Leverage**: `integrations/mcp-server-semantic-analysis/src/agents/coordinator.ts` workflow patterns\n- **Requirements**: FR-6.1, FR-6.2\n- **Prompt**: Role: AI/ML Engineer specializing in NLP and knowledge extraction systems | Task: Create StreamingKnowledgeExtractor agent in TypeScript that (1) monitors live session transcripts via LSL system, (2) buffers observations to reduce LLM calls (configurable buffer size, default 10 observations), (3) extracts code patterns, implementation decisions, and problem-solution pairs, (4) classifies observations by domain (frontend, backend, infrastructure, etc.), (5) assigns confidence scores to extractions, (6) uses UnifiedInferenceEngine for LLM analysis, extending the coordinator workflow patterns for streaming operation | Restrictions: Must not block live development session; must handle partial/incomplete transcripts; must deduplicate similar observations; confidence threshold >0.7 for storage; must respect budget limits | Success: Extracts knowledge from live sessions without blocking, buffering reduces LLM calls >80%, extractions have >90% relevance, classifications accurate >85%, confidence scores reliable, budget limits respected\n\n### [ ] 3.2 Create ConceptAbstractionAgent\n\n- **File**: `integrations/mcp-server-semantic-analysis/src/agents/ConceptAbstractionAgent.ts`\n- **Purpose**: Generalize specific observations into reusable concepts with implementations, tradeoffs, and pitfalls\n- **Leverage**: Existing agent patterns, concept abstraction techniques from requirements\n- **Requirements**: FR-6.3, FR-6.4\n- **Prompt**: Role: Knowledge Engineer with expertise in concept modeling and ontology design | Task: Implement ConceptAbstractionAgent in TypeScript that (1) analyzes observations to identify recurring patterns across sessions/projects, (2) abstracts patterns into reusable concepts with clear definitions, (3) generates implementations list (how to apply concept), (4) identifies tradeoffs (pros/cons of concept), (5) documents common pitfalls (mistakes to avoid), (6) links related concepts to build knowledge graph, (7) validates concepts have >3 supporting observations before creation, using UnifiedInferenceEngine for generalization | Restrictions: Must not create concepts from single observation; must validate concept quality before storage; must link to source observations for traceability; must support concept evolution as more observations added | Success: Concepts are well-formed and reusable, >3 observations per concept, implementations actionable, tradeoffs balanced, pitfalls accurate, related concepts linked correctly, concept quality >85%\n\n### [ ] 3.3 Create KnowledgeStorageService\n\n- **File**: `src/knowledge-management/KnowledgeStorageService.js`\n- **Purpose**: Store and retrieve knowledge (concepts, patterns, observations) in Qdrant + DuckDB\n- **Leverage**: DatabaseManager, QdrantCollectionManager, DuckDBSchemaManager\n- **Requirements**: FR-6.5, FR-6.6\n- **Prompt**: Role: Backend Developer with expertise in data persistence and retrieval systems | Task: Implement KnowledgeStorageService that (1) stores concepts in Qdrant knowledge_concepts collection with 1024-dim embeddings, (2) stores code patterns in code_patterns collection with 384-dim embeddings, (3) tracks knowledge events in DuckDB knowledge_events table, (4) supports semantic search by concept similarity, (5) supports temporal queries (recent knowledge, trending concepts), (6) implements deduplication to prevent storing similar knowledge, (7) provides batch operations for efficiency, leveraging DatabaseManager for unified access | Restrictions: Must prevent duplicate knowledge storage (similarity >0.95); must handle concurrent writes safely; must support transactional updates; must provide rollback on failure | Success: Knowledge stored correctly in both databases, semantic search returns relevant results >90%, temporal queries execute <500ms, deduplication works >95% accuracy, concurrent writes safe, transactions reliable\n\n### [ ] 3.4 Create KnowledgeDecayTracker\n\n- **File**: `src/knowledge-management/KnowledgeDecayTracker.js`\n- **Purpose**: Track knowledge freshness and mark stale/deprecated knowledge\n- **Leverage**: DuckDB temporal queries, knowledge access patterns\n- **Requirements**: FR-6.7, FR-6.8\n- **Prompt**: Role: Data Scientist with expertise in temporal analysis and knowledge lifecycle management | Task: Implement KnowledgeDecayTracker that (1) tracks knowledge access patterns in DuckDB, (2) calculates freshness scores based on recency of use and creation date, (3) classifies knowledge as fresh (<30 days), aging (30-90 days), stale (90-180 days), deprecated (>180 days unused), (4) adjusts search ranking based on freshness, (5) suggests knowledge for review/deprecation, (6) runs decay analysis daily in background, using DuckDB temporal queries for efficient analysis | Restrictions: Must not delete knowledge automatically (only mark as deprecated); must consider context-specific freshness (some knowledge timeless); decay analysis must complete <5 minutes; must not impact live queries | Success: Freshness scores accurate and meaningful, classifications reflect actual knowledge lifecycle, search ranking improved by freshness weighting, deprecation suggestions reviewed and accurate >80%, background analysis efficient\n\n---\n\n## Phase 4: Enhanced Trajectory System\n\n### [ ] 4.1 Extend RealTimeTrajectoryAnalyzer with intent classification\n\n- **File**: `src/live-logging/RealTimeTrajectoryAnalyzer.js` (extend existing)\n- **Purpose**: Add intent classification (learning, debugging, feature-dev, refactoring, testing) to existing trajectory states\n- **Leverage**: Existing trajectory analyzer, UnifiedInferenceEngine\n- **Requirements**: FR-7.1, FR-7.2\n- **Prompt**: Role: System Architect specializing in behavior analysis and intent recognition | Task: Extend RealTimeTrajectoryAnalyzer by (1) adding intent classification alongside existing trajectory states, (2) classifying session intent as learning/debugging/feature-dev/refactoring/testing/exploration, (3) extracting session goal from conversation context, (4) identifying active concepts being used/learned, (5) tracking intent transitions over session timeline, (6) sharing UnifiedInferenceEngine with knowledge system (no code duplication), (7) maintaining backward compatibility with existing trajectory states | Restrictions: Must not break existing trajectory functionality; must complete intent classification in <2s; must handle ambiguous intents gracefully; must persist intent history for analysis | Success: Intent classification accurate >85%, existing trajectory features still work, inference engine shared correctly, intent transitions tracked, session goals identified correctly, backward compatible\n\n### [ ] 4.2 Add concept extraction to trajectory analysis\n\n- **File**: `src/live-logging/TrajectoryConceptExtractor.js`\n- **Purpose**: Extract concepts being used/learned during development session for trajectory context\n- **Leverage**: KnowledgeStorageService for concept lookup, UnifiedInferenceEngine\n- **Requirements**: FR-7.3, FR-7.4\n- **Prompt**: Role: Knowledge Graph Engineer with expertise in entity extraction and linking | Task: Create TrajectoryConceptExtractor that (1) monitors conversation for concept mentions (patterns, technologies, architectures), (2) links mentions to existing concepts in knowledge base via semantic search, (3) tracks new concepts being learned vs. existing concepts being applied, (4) provides concept context to trajectory analyzer for better state classification, (5) updates concept access timestamps for decay tracking, using KnowledgeStorageService for concept lookup and UnifiedInferenceEngine for extraction | Restrictions: Must not slow down trajectory updates (async processing acceptable); must handle concept ambiguity (multiple matches); must distinguish between casual mention vs. active use; must support concept evolution | Success: Concepts extracted with >80% accuracy, linked to knowledge base correctly, learning vs. application distinguished, trajectory context improved, concept access tracked, processing completes in <1s\n\n### [ ] 4.3 Enhance trajectory state schema\n\n- **File**: `.specstory/trajectory/live-state.json` schema update\n- **Purpose**: Update trajectory state schema to include intent, goal, concepts, confidence\n- **Leverage**: Existing live-state.json schema\n- **Requirements**: FR-7.5\n- **Prompt**: Role: Data Modeling Specialist with expertise in schema design and versioning | Task: Extend .specstory/trajectory/live-state.json schema to (1) add intent field (learning/debugging/feature-dev/refactoring/testing/exploration), (2) add goal field (extracted session objective), (3) add concepts array (active concepts being used/learned), (4) add confidence score for trajectory classification, (5) add intent_history array for tracking intent transitions, (6) maintain backward compatibility with existing trajectory tools, (7) support schema versioning for future updates | Restrictions: Must not break existing trajectory visualization; must support gradual migration from old schema; must validate schema changes don't cause data loss; must document schema changes | Success: Schema extended correctly, backward compatible, new fields validated, migration path clear, visualization tools updated, documentation complete\n\n### [ ] 4.4 Store trajectory history in DuckDB\n\n- **File**: `src/live-logging/TrajectoryHistoryService.js`\n- **Purpose**: Persist trajectory history to DuckDB for cross-session analysis\n- **Leverage**: DuckDBSchemaManager, trajectory_history table\n- **Requirements**: FR-7.6\n- **Prompt**: Role: Data Engineer with expertise in time-series data and historical analysis | Task: Implement TrajectoryHistoryService that (1) persists trajectory state changes to DuckDB trajectory_history table, (2) supports queries for trajectory patterns across sessions (common intents, typical flows), (3) enables cross-session learning (what worked before in similar contexts), (4) provides trajectory analytics (time in each state, success patterns), (5) supports efficient queries by project, session, intent, (6) implements data retention policies (archive old trajectories), using DuckDB for efficient temporal storage | Restrictions: Must not impact live trajectory updates (async persistence); must handle high write volume efficiently; must support flexible querying; must implement data archival for old sessions | Success: Trajectory history persisted correctly, queries execute <500ms, cross-session patterns identified, analytics meaningful, retention policies work, async persistence reliable\n\n---\n\n## Phase 5: Integration & Migration\n\n### [ ] 5.1 Create migration script for JSON to DuckDB/Qdrant\n\n- **File**: `scripts/migrate-knowledge-to-databases.js`\n- **Purpose**: Migrate existing JSON knowledge base to Qdrant + DuckDB without breaking VKB visualization\n- **Leverage**: Existing JSON knowledge structure, DatabaseManager\n- **Requirements**: FR-8.1, FR-8.2\n- **Prompt**: Role: Database Migration Specialist with expertise in data transformation and migration | Task: Create migration script that (1) reads existing shared-memory-*.json files, (2) transforms knowledge to new schema (concepts, patterns, events), (3) generates embeddings for all knowledge items, (4) loads knowledge into Qdrant collections and DuckDB tables, (5) preserves entity IDs and relationships for VKB compatibility, (6) supports incremental migration (partial loads), (7) validates migration correctness (data integrity checks), (8) provides rollback mechanism if migration fails | Restrictions: Must not delete original JSON files until migration validated; must handle large JSON files efficiently; must preserve all knowledge relationships; must validate embedding quality | Success: All knowledge migrated correctly, entity IDs preserved, relationships maintained, VKB visualization works with new backend, migration reversible, data integrity validated\n\n### [ ] 5.2 Adapt VKB visualization for database backend\n\n- **File**: `scripts/vkb-visualizer.js` (extend existing)\n- **Purpose**: Update VKB visualization to read from Qdrant/DuckDB instead of JSON files\n- **Leverage**: Existing VKB visualization code, DatabaseManager\n- **Requirements**: FR-8.3\n- **Prompt**: Role: Full-stack Developer with expertise in data visualization and graph rendering | Task: Adapt VKB visualizer to (1) query knowledge from Qdrant/DuckDB instead of JSON files, (2) transform database results to VKB graph format, (3) support same visualization features (node types, relationships, search), (4) add new features (freshness visualization, concept clustering), (5) maintain backward compatibility with existing VKB command, (6) optimize queries for large knowledge bases, extending existing visualization code with database backend | Restrictions: Must not break existing VKB workflows; must handle large graphs efficiently; must maintain visual consistency; queries must execute <2s for rendering | Success: VKB visualization works with new backend, all existing features functional, new features integrated, performance acceptable, backward compatible\n\n### [ ] 5.3 Create agent adapter integration tests\n\n- **File**: `tests/integration/agent-adapters.test.js`\n- **Purpose**: Test knowledge system works with different coding agents (Claude, CoPilot, others)\n- **Leverage**: Existing agent adapter patterns from docs/agent-integration-guide.md\n- **Requirements**: FR-9.1, FR-9.2\n- **Prompt**: Role: QA Engineer specializing in integration testing and multi-agent systems | Task: Create integration tests that (1) verify AgentAgnosticCache works with file/HTTP/MCP backends, (2) test knowledge extraction with different agent transcript formats, (3) validate trajectory tracking across different agents, (4) ensure budget tracking works regardless of agent, (5) test graceful degradation when agent-specific features unavailable, (6) simulate multi-agent scenarios (team with mixed agents), leveraging agent adapter patterns from integration guide | Restrictions: Must not require all agents installed for tests (use mocks); must test edge cases (network failures, partial transcripts); must validate cross-agent knowledge sharing; must ensure tests run in CI/CD | Success: All agent adapters tested, knowledge extraction works across agents, trajectory tracking agent-agnostic, budget enforcement universal, graceful degradation verified, tests reliable in CI/CD\n\n### [ ] 5.4 Create performance benchmarks\n\n- **File**: `tests/performance/knowledge-system-benchmarks.js`\n- **Purpose**: Benchmark system performance against requirements (inference <2s, queries <500ms, etc.)\n- **Leverage**: Existing performance testing patterns\n- **Requirements**: NFR-1, NFR-2, NFR-3\n- **Prompt**: Role: Performance Engineer with expertise in benchmarking and optimization | Task: Create performance benchmarks that (1) measure inference latency (target <2s p95), (2) measure database query performance (target <500ms p95), (3) measure embedding generation speed (384-dim <50ms, 1024-dim <200ms), (4) measure cache hit rates (target >40%), (5) measure budget tracking overhead (<10ms), (6) measure end-to-end knowledge extraction pipeline, (7) identify performance bottlenecks and optimization opportunities | Restrictions: Must run on realistic data volumes; must measure p50, p95, p99 latencies; must test under load (concurrent operations); must provide actionable optimization recommendations | Success: Benchmarks comprehensive and realistic, performance meets requirements, bottlenecks identified, optimization recommendations actionable, benchmarks run in CI/CD\n\n---\n\n## Phase 6: Testing & Quality Assurance\n\n### [ ] 6.1 Create unit tests for UnifiedInferenceEngine\n\n- **File**: `tests/unit/inference/UnifiedInferenceEngine.test.js`\n- **Purpose**: Test inference engine with mocked providers\n- **Leverage**: Jest testing framework, provider mocks\n- **Requirements**: All FR-1 requirements\n- **Prompt**: Role: QA Engineer with expertise in unit testing and mocking frameworks | Task: Create comprehensive unit tests for UnifiedInferenceEngine covering (1) provider routing logic (local vs. remote based on privacy/budget), (2) circuit breaker behavior (failover on provider errors), (3) caching functionality (cache hits/misses), (4) budget enforcement (blocks when exceeded), (5) sensitivity routing (sensitive data to local models), (6) streaming response handling, (7) error handling and recovery, using mocked providers to isolate inference engine logic | Restrictions: Must not make real API calls; must test all code paths; must verify error conditions; must test concurrent requests | Success: All inference engine logic tested, >90% code coverage, mocks realistic, error cases covered, concurrent scenarios tested, tests run fast (<5s)\n\n### [ ] 6.2 Create unit tests for knowledge extraction\n\n- **File**: `tests/unit/knowledge-management/KnowledgeExtraction.test.ts`\n- **Purpose**: Test knowledge extraction and concept abstraction\n- **Leverage**: TypeScript testing framework, sample transcripts\n- **Requirements**: All FR-6 requirements\n- **Prompt**: Role: QA Engineer with expertise in TypeScript testing and NLP systems | Task: Create TypeScript unit tests for knowledge extraction covering (1) observation extraction from transcripts, (2) observation buffering and batching, (3) pattern detection across observations, (4) concept abstraction logic, (5) confidence scoring, (6) deduplication, (7) knowledge graph construction, using sample transcripts and mocked LLM responses | Restrictions: Must test with realistic transcript samples; must verify concept quality requirements (>3 observations); must test edge cases (incomplete transcripts, ambiguous patterns); must validate knowledge graph integrity | Success: All extraction logic tested, concept quality validated, edge cases covered, knowledge graph integrity verified, confidence scores reliable, tests comprehensive\n\n### [ ] 6.3 Create integration tests for database operations\n\n- **File**: `tests/integration/databases/DatabaseOperations.test.js`\n- **Purpose**: Test Qdrant + DuckDB operations with real databases\n- **Leverage**: Test databases (Qdrant in-memory, DuckDB temp file)\n- **Requirements**: All FR-5 requirements\n- **Prompt**: Role: Database QA Engineer with expertise in database testing and data integrity | Task: Create integration tests for database operations covering (1) Qdrant vector search accuracy, (2) DuckDB temporal queries, (3) concurrent read/write operations, (4) transaction semantics across databases, (5) data consistency after failures, (6) query performance under load, (7) schema migrations, using test instances of Qdrant and DuckDB | Restrictions: Must use isolated test databases; must clean up after tests; must test failure scenarios (database down, connection loss); must validate data integrity | Success: All database operations tested, search accuracy validated, concurrent operations safe, transactions reliable, failure handling works, performance acceptable, tests isolated\n\n### [ ] 6.4 Create end-to-end workflow tests\n\n- **File**: `tests/e2e/knowledge-learning-workflow.test.js`\n- **Purpose**: Test complete workflow from session transcript to knowledge retrieval\n- **Leverage**: Sample development sessions, full system integration\n- **Requirements**: All functional requirements\n- **Prompt**: Role: QA Automation Engineer with expertise in end-to-end testing and workflow validation | Task: Create E2E tests for complete knowledge learning workflow covering (1) live session monitoring and transcript processing, (2) knowledge extraction from conversations, (3) concept abstraction and storage, (4) knowledge retrieval and search, (5) trajectory tracking with intent classification, (6) budget tracking and enforcement, (7) cross-session knowledge sharing, simulating realistic development sessions end-to-end | Restrictions: Must use realistic session scenarios; must validate end-user value (knowledge actually useful); must test multi-session scenarios; must verify cross-agent compatibility | Success: Complete workflow tested end-to-end, knowledge extraction produces useful results, retrieval finds relevant knowledge, trajectory tracking accurate, budget enforcement works, cross-session learning validated, tests reflect real usage\n\n---\n\n## Phase 7: Documentation & Deployment\n\n### [ ] 7.1 Create developer documentation\n\n- **File**: `docs/knowledge-management/continuous-learning-system.md`\n- **Purpose**: Document system architecture, configuration, and usage for developers\n- **Leverage**: Existing docs structure, design document\n- **Requirements**: All requirements\n- **Prompt**: Role: Technical Writer with expertise in developer documentation and system architecture | Task: Create comprehensive developer documentation covering (1) system architecture overview with diagrams, (2) configuration guide (budget limits, sensitivity topics, provider selection), (3) usage examples for knowledge extraction and retrieval, (4) integration guide for new coding agents, (5) troubleshooting common issues, (6) performance tuning recommendations, (7) API reference for all components, based on design document and implementation | Restrictions: Must include code examples; must provide clear configuration instructions; must explain architectural decisions; must document all APIs | Success: Documentation comprehensive and clear, developers can configure and use system, troubleshooting guide helpful, API reference complete, examples functional\n\n### [ ] 7.2 Create operator documentation\n\n- **File**: `docs/operations/knowledge-system-ops.md`\n- **Purpose**: Document operational procedures (monitoring, maintenance, scaling)\n- **Leverage**: Existing operational docs\n- **Requirements**: NFR requirements\n- **Prompt**: Role: DevOps Engineer with expertise in operational documentation and system administration | Task: Create operational documentation covering (1) system health monitoring (metrics, alerts), (2) database maintenance (backups, migrations, indexing), (3) cost monitoring and optimization, (4) scaling guidelines (when/how to scale), (5) disaster recovery procedures, (6) performance tuning, (7) troubleshooting production issues, based on operational requirements | Restrictions: Must provide actionable procedures; must include monitoring setup; must document backup/recovery; must explain scaling decisions | Success: Operations team can monitor and maintain system, health metrics tracked, backups automated, scaling procedures clear, troubleshooting effective, documentation actionable\n\n### [ ] 7.3 Create configuration templates\n\n- **File**: `.specstory/config/knowledge-system.template.json`, `sensitivity-topics.template.json`\n- **Purpose**: Provide configuration templates for projects to customize system behavior\n- **Leverage**: Design document configuration examples\n- **Requirements**: FR-10 (configurability)\n- **Prompt**: Role: Configuration Management Specialist with expertise in JSON schemas and templating | Task: Create configuration templates for (1) knowledge system settings (budget limits, provider priorities, caching settings), (2) sensitivity topics (custom privacy rules per project), (3) embedding models (384-dim vs 1024-dim selection), (4) decay policies (freshness thresholds), (5) agent-specific settings, with clear comments explaining each setting and reasonable defaults | Restrictions: Must validate against JSON schema; must include inline documentation; must provide examples for common scenarios; must set safe defaults | Success: Templates comprehensive and documented, defaults safe and reasonable, examples clear, projects can customize easily, validation catches errors\n\n### [ ] 7.4 Create deployment checklist\n\n- **File**: `docs/deployment/knowledge-system-checklist.md`\n- **Purpose**: Checklist for deploying knowledge system to new projects/teams\n- **Leverage**: Deployment best practices\n- **Requirements**: All requirements\n- **Prompt**: Role: Release Manager with expertise in deployment planning and system rollout | Task: Create deployment checklist covering (1) prerequisites (database setup, API keys, system requirements), (2) installation steps (dependencies, configuration, database init), (3) migration procedures (JSON to database), (4) verification steps (health checks, test queries), (5) rollback procedures if issues occur, (6) team training requirements, (7) go-live criteria, ensuring smooth deployment to production | Restrictions: Must be step-by-step with verification at each step; must include rollback plan; must define clear go/no-go criteria; must consider team readiness | Success: Deployment checklist complete and actionable, all steps verified, rollback plan tested, go-live criteria clear, teams prepared, deployments successful\n\n---\n\n## Phase 8: Final Integration & Validation\n\n### [ ] 8.1 Integrate with existing LSL system\n\n- **File**: `scripts/enhanced-transcript-monitor.js` (extend existing)\n- **Purpose**: Integrate knowledge extraction into live session logging pipeline\n- **Leverage**: Existing LSL monitoring, StreamingKnowledgeExtractor\n- **Requirements**: FR-11.1, FR-11.2\n- **Prompt**: Role: Integration Engineer with expertise in event-driven systems and real-time processing | Task: Integrate knowledge extraction into LSL pipeline by (1) adding knowledge extraction hooks to transcript monitor, (2) triggering StreamingKnowledgeExtractor on new transcript events, (3) ensuring knowledge extraction doesn't block LSL processing, (4) handling errors gracefully (knowledge extraction failures don't break LSL), (5) providing status visibility (knowledge extraction progress in status line), extending existing LSL monitoring system | Restrictions: Must not break existing LSL functionality; must process events asynchronously; must handle high transcript volume; must provide clear error messages | Success: Knowledge extraction integrated with LSL, processing asynchronous and non-blocking, errors handled gracefully, status visible, existing LSL features unaffected, high-volume handling validated\n\n### [ ] 8.2 Update StatusLine with knowledge system metrics\n\n- **File**: `scripts/combined-status-line.js` (extend existing)\n- **Purpose**: Add knowledge system health and metrics to status line display\n- **Leverage**: Existing status line system\n- **Requirements**: NFR-4 (observability)\n- **Prompt**: Role: Frontend Developer with expertise in CLI interfaces and real-time monitoring | Task: Extend status line to display (1) knowledge extraction status (idle/processing/error), (2) budget usage percentage for current month, (3) cache hit rate, (4) recent concepts learned, (5) trajectory intent and confidence, (6) database health (Qdrant/DuckDB status), (7) color-coded indicators for system health, extending existing status line system with knowledge metrics | Restrictions: Must not clutter status line (concise display); must update in real-time; must handle metric unavailability gracefully; must be visually consistent | Success: Status line shows knowledge system health clearly, metrics accurate and real-time, display concise and informative, color coding intuitive, consistent with existing status line\n\n### [ ] 8.3 Create acceptance test scenarios\n\n- **File**: `tests/acceptance/knowledge-system-acceptance.test.js`\n- **Purpose**: Validate system meets all user stories and success criteria\n- **Leverage**: Requirements document user stories\n- **Requirements**: All user stories (US-1 through US-6)\n- **Prompt**: Role: Product QA Engineer with expertise in acceptance testing and user story validation | Task: Create acceptance tests validating all user stories covering (1) US-1: Tool interaction learning from repeated patterns, (2) US-2: Architecture decisions captured with context, (3) US-3: Cross-session knowledge reuse, (4) US-4: Local model deployment for sensitive data, (5) US-5: Budget tracking and cost optimization, (6) US-6: Multi-agent team knowledge sharing, simulating real user scenarios and verifying success criteria | Restrictions: Must test from user perspective (not implementation details); must validate actual user value; must cover all success criteria; must use realistic scenarios | Success: All user stories validated, success criteria met, user value demonstrated, scenarios realistic, tests pass consistently, system ready for production\n\n### [ ] 8.4 Performance optimization based on benchmarks\n\n- **File**: Multiple files based on bottlenecks identified in 5.4\n- **Purpose**: Optimize system performance to meet all non-functional requirements\n- **Leverage**: Performance benchmark results\n- **Requirements**: All NFR requirements\n- **Prompt**: Role: Performance Optimization Engineer with expertise in profiling and tuning | Task: Optimize system performance based on benchmark results by (1) addressing identified bottlenecks (slow queries, high latency operations), (2) tuning cache configurations for higher hit rates, (3) optimizing database queries and indexes, (4) improving embedding generation throughput, (5) reducing inference latency through batching/parallelization, (6) minimizing memory usage and preventing leaks, (7) validating optimizations meet all performance requirements | Restrictions: Must not sacrifice correctness for performance; must measure improvement after each optimization; must document optimization rationale; must test performance under load | Success: All performance requirements met (inference <2s, queries <500ms, etc.), bottlenecks eliminated, optimizations validated with benchmarks, system performant under realistic load, documentation updated\n\n### [ ] 8.5 Security review and hardening\n\n- **File**: Multiple files for security improvements\n- **Purpose**: Security review of system focusing on data privacy and API key management\n- **Leverage**: Security best practices, privacy requirements\n- **Requirements**: NFR-5 (privacy and security)\n- **Prompt**: Role: Security Engineer with expertise in application security and privacy protection | Task: Conduct security review and implement hardening covering (1) sensitive data handling (verify no leaks to remote APIs), (2) API key management (secure storage, rotation), (3) authentication for cache backends (HTTP/MCP), (4) input validation for all user/transcript data, (5) SQL injection prevention (DuckDB queries), (6) dependency vulnerability scanning, (7) data encryption at rest/in transit, ensuring privacy and security requirements met | Restrictions: Must not break existing functionality; must validate all security controls; must document security architecture; must test security measures | Success: No sensitive data leaks validated, API keys secure, authentication enforced, inputs validated, SQL injection prevented, dependencies secure, encryption implemented, privacy requirements met\n\n### [ ] 8.6 Final system validation and sign-off\n\n- **File**: `.spec-workflow/specs/continuous-learning-knowledge-system/validation-report.md`\n- **Purpose**: Comprehensive system validation and sign-off document\n- **Leverage**: All tests, benchmarks, and acceptance criteria\n- **Requirements**: All requirements\n- **Prompt**: Role: Quality Assurance Lead with expertise in system validation and release management | Task: Create comprehensive validation report covering (1) requirements traceability (all requirements implemented and tested), (2) test results summary (unit, integration, E2E, acceptance, performance), (3) known issues and limitations, (4) deployment readiness assessment, (5) risk analysis for production rollout, (6) recommendations for go-live or additional work needed, (7) sign-off checklist for stakeholders, validating system is production-ready | Restrictions: Must be objective and comprehensive; must document all test results; must identify any gaps; must provide clear go/no-go recommendation | Success: Validation report complete and accurate, all requirements traced to tests, test results documented, issues identified and triaged, deployment readiness assessed, go-live decision clear, stakeholders can make informed decision\n\n---\n\n## Task Tracking\n\nTasks follow this workflow:\n- `[ ]` - Pending\n- `[-]` - In Progress\n- `[x]` - Completed\n\nUpdate task status as work progresses. Each completed task should be verified before marking complete.\n\n---\n\n## Implementation Notes\n\n### Dependencies Between Tasks\n\n- **Phase 1 (Foundation)** must complete before Phase 2 (Database) and Phase 3 (Knowledge)\n- **Phase 2 (Database)** must complete before Phase 3 (Knowledge) can fully function\n- **Phase 4 (Trajectory)** can proceed in parallel with Phase 3 after Phase 1 completes\n- **Phase 5 (Migration)** requires Phase 2 and Phase 3 complete\n- **Phase 6 (Testing)** can begin incrementally after each component but needs Phase 5 complete for E2E tests\n- **Phase 7 (Documentation)** can proceed in parallel with implementation\n- **Phase 8 (Final Integration)** requires all previous phases complete\n\n### Resource Allocation\n\n- **Backend Engineers**: Phase 1, 2, 3, 5\n- **TypeScript/Agent Developers**: Phase 3, 4\n- **QA Engineers**: Phase 6, 8\n- **DevOps Engineers**: Phase 5, 7, 8\n- **Technical Writers**: Phase 7\n\n### Estimated Timeline\n\n- **Phase 1**: 2 weeks (critical path)\n- **Phase 2**: 1 week (after Phase 1)\n- **Phase 3**: 2 weeks (after Phase 1 & 2)\n- **Phase 4**: 1 week (after Phase 1)\n- **Phase 5**: 1 week (after Phase 2 & 3)\n- **Phase 6**: 2 weeks (incremental, completes after Phase 5)\n- **Phase 7**: 1 week (parallel with implementation)\n- **Phase 8**: 1 week (after all other phases)\n\n**Total**: ~8-10 weeks with parallel work\n\n---\n\n## Success Criteria\n\nThe implementation is considered successful when:\n\n1. ✅ All tasks marked complete with verification\n2. ✅ All unit tests passing (>90% coverage)\n3. ✅ All integration tests passing\n4. ✅ All E2E tests passing\n5. ✅ All acceptance tests passing (user stories validated)\n6. ✅ Performance benchmarks meet requirements\n7. ✅ Security review completed with no critical issues\n8. ✅ Documentation complete and reviewed\n9. ✅ Migration from JSON to databases successful\n10. ✅ System deployed and validated in production\n\n---\n\n**Next Step**: Begin Phase 1 - Foundation with task 1.1 (UnifiedInferenceEngine)\n",
  "fileStats": {
    "size": 47286,
    "lines": 397,
    "lastModified": "2025-10-18T20:08:35.335Z"
  },
  "comments": []
}